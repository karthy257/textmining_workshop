---
title: "4. Tweet Text Mining"
author: "Michael Weisner"
date: "3/8/2019"
output: html_document
---

This tutorial is heavily based on the tutorial written by Leah Wasser and Carson Farmer, [https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/text-mining-twitter-data-intro-r/](here).

# Searching for Tweets Related to Climate

## Packages
```{r, eval = FALSE}
# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(igraph)
library(ggraph)
library(ggthemes)
```


## How to get data with Twitter API
```{r, eval = FALSE}
#climate_tweets <- search_tweets(q = "#climatechange", n = 10000,
#                                      lang = "en",
#                                      include_rts = FALSE)
#save_as_csv(x = climate_tweets, "climate_tweets.csv", prepend_ids = TRUE, na = "NA",
#  fileEncoding = "UTF-8")
```

## Pre-found tweets about Climate Change
```{r}
climate_tweets <- read_csv("./climate_tweets.csv")
head(climate_tweets$text)
```

# Cleaning Tweets
```{r}
# remove urls tidyverse is failing here for some reason
#climate_tweets <- climate_tweets %>%
#  mutate_at(c("stripped_text"), gsub("http.*","",.))

 climate_tweets$stripped_text <- gsub("http.*","",  climate_tweets$text)
 climate_tweets$stripped_text <- gsub("https.*","", climate_tweets$stripped_text)
```

## Unnest tokens with stripped text
```{r}
# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_clean <- climate_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
```


## Remove Stop Words

Just so it's clear, here's what the top 15 words would be if we don't remove stop words.

```{r}
climate_tweets_clean %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets") +
  theme_tufte(ticks = TRUE)
```

Let's see how many words are in the data already based on the number of rows

```{r}
nrow(climate_tweets_clean)
```

Now let's load the stop_words `tidytext()` package and remove them from our data.
```{r}
# load list of stop words - from the tidytext package
data("stop_words")

# remove stop words from your list of words
cleaned_tweet_words <- climate_tweets_clean %>%
  anti_join(stop_words)
```

There should now be fewer words
```{r}
# there should be fewer words now
nrow(cleaned_tweet_words)
```

Let's plot the top 15 words
```{r}
# plot the top 15 words
cleaned_tweet_words %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets") +
  theme_tufte(ticks = TRUE)
```

# Networks of Words (ngrams)

ngrams specifies pairs and 2 is the number of words together

```{r}
# library(devtools)
# install_github("dgrtwo/widyr")
library(widyr)

# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_paired_words <- climate_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)

climate_tweets_paired_words %>%
  count(paired_words, sort = TRUE)
```

```{r}
library(tidyr)
climate_tweets_separated_words <- climate_tweets_paired_words %>%
  separate(paired_words, c("word1", "word2"), sep = " ")

climate_tweets_filtered <- climate_tweets_separated_words %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
climate_words_counts <- climate_tweets_filtered %>%
  count(word1, word2, sort = TRUE)

head(climate_words_counts)
```

Finally let's plot the data

```{r}
library(igraph)
library(ggraph)

# plot climate change word network
climate_words_counts %>%
        filter(n >= 24) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "fr") +
        geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = "darkslategray4", size = 3) +
        geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
        labs(title = "Word Network: Tweets using the hashtag - Climate Change",
             subtitle = "Text mining twitter data ",
             x = "", y = "") +
  theme_tufte(ticks = TRUE)
```





---
title: "4. Tweet Text Mining"
author: "Michael Weisner"
date: "3/8/2019"
output: html_document
---

This tutorial is heavily based on the tutorial written by Leah Wasser and Carson Farmer, [https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/text-mining-twitter-data-intro-r/](here).

# Searching for Tweets Related to Climate

## Packages
```{r, eval = FALSE}
# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(igraph)
library(ggraph)
library(ggthemes)
```

## How to get data with Twitter API

In the interest of time we are going to skip to pre-collected tweets, but I will include information on how to scrape yourself. 

For a step by step process on scraping tweets, please see [rtweet.info](https://rtweet.info)
All users must be authorized to interact with Twitter’s APIs. To become authorized, follow the instructions below to (1) make a Twitter app and (2) create and save your access token (using one of the two authorization methods described below).

### Create an app
To gain access Twitter’s APIs, first go to apps.twitter.com and create a new app by completing the form fields (note: users must enter the value for Callback URL exactly as it appears below):

+ Name: Name of Twitter app e.g., my_twitter_research_app
+ Description: Describe use case e.g., for researching trends and behaviors on twitter
+ Website: Valid website e.g., https://twitter.com/kearneymw
+ ***Callback URL***: http://127.0.0.1:1410
+ Check yes if you agree and then click “Create your Twitter application”

### Authenticate via web browser (interactive)
Go to your app’s page at apps.twitter.com and click the tab labeled Keys and Access Tokens
Copy the Consumer Key and Consumer Secret values and pass them, along with the name of your app, to the create_token() function

```{r, eval = FALSE}
# ## web browser method: create token and save it as an environment variable
# create_token(
#   app = "my_twitter_research_app",
#   consumer_key = "XYznzPFOFZR2a39FwWKN1Jp41",
#   consumer_secret = "CtkGEWmSevZqJuKl6HHrBxbCybxI1xGLqrD5ynPd9jG0SoHZbD")
```

## Scrape Twitter API
If people are interested I can show you how to pre-scrape tweets.
```{r}
#climate_tweets <- search_tweets(q = "#climatechange", n = 10000,
#                                      lang = "en",
#                                      include_rts = FALSE)
#save_as_csv(x = climate_tweets, "climate_tweets.csv", prepend_ids = TRUE, na = "NA",
#  fileEncoding = "UTF-8")
```

## Pre-scraped tweets about Climate Change
For this exercise we will use pre-scraped tweets
```{r}
library(tidyverse)
climate_tweets <- read_csv("https://raw.githubusercontent.com/mdweisner/textmining_workshop/master/climate_tweets.csv")

# OR

# climate_tweets <- read_csv("https://bit.ly/2IBle5W")
head(climate_tweets)
```

# Cleaning Tweets
```{r}
library(tidytext)
# remove urls tidyverse is failing here for some reason
#climate_tweets <- climate_tweets %>%
#  mutate_at(c("stripped_text"), gsub("http.*","",.))

 climate_tweets$stripped_text <- gsub("http.*","",  climate_tweets$text)
 climate_tweets$stripped_text <- gsub("https.*","", climate_tweets$stripped_text)
```

## Unnest tokens with stripped text
```{r}
# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_clean <- climate_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
```


## Remove Stop Words

Just so it's clear, here's what the top 15 words would be if we don't remove stop words.

```{r}
library(ggplot2)
library(ggthemes)
climate_tweets_clean %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets") +
  theme_tufte(ticks = TRUE)
```

Let's see how many words are in the data already based on the number of rows

```{r}
nrow(climate_tweets_clean)
```

Now let's load the stop_words `tidytext()` package and remove them from our data.
```{r}
# load list of stop words - from the tidytext package
data("stop_words")

# remove stop words from your list of words
cleaned_tweet_words <- climate_tweets_clean %>%
  anti_join(stop_words)
```

There should now be fewer words
```{r}
# there should be fewer words now
nrow(cleaned_tweet_words)
```

Let's plot the top 15 words
```{r}
# plot the top 15 words
cleaned_tweet_words %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets") +
  theme_tufte(ticks = TRUE)
```

# Networks of Words (ngrams)

ngrams specifies pairs and 2 is the number of words together

```{r}
#library(devtools)
# devtools::install_github("dgrtwo/widyr")
library(widyr)

# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_paired_words <- climate_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)

climate_tweets_paired_words %>%
  count(paired_words, sort = TRUE)
```

```{r}
library(tidyr)
climate_tweets_separated_words <- climate_tweets_paired_words %>%
  separate(paired_words, c("word1", "word2"), sep = " ")

climate_tweets_filtered <- climate_tweets_separated_words %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
climate_words_counts <- climate_tweets_filtered %>%
  count(word1, word2, sort = TRUE)

head(climate_words_counts)
```

Finally let's plot the data

```{r}
library(igraph)
library(ggraph)

# plot climate change word network
climate_words_counts %>%
        filter(n >= 24) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "fr") +
        geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = "darkslategray4", size = 3) +
        geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
        labs(title = "Word Network: Tweets using the hashtag - Climate Change",
             subtitle = "Text mining twitter data ",
             x = "", y = "") +
  theme_tufte(ticks = TRUE)
```




